{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = datasets.load_dataset('merionum/ru_paraphraser')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'id_1', 'id_2', 'text_1', 'text_2', 'class'],\n        num_rows: 7227\n    })\n    test: Dataset({\n        features: ['id', 'id_1', 'id_2', 'text_1', 'text_2', 'class'],\n        num_rows: 1924\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model_name = \"IlyaGusev/xlm_roberta_large_headline_cause_simple\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "label_list = sorted(set(data['train']['class']))\n",
    "labels2id = { key:id for id, key in enumerate(label_list)}\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, labels2id):\n",
    "    def tokenize_and_align_labels_(examples):\n",
    "#         tokenized_inputs = tokenizer([[text_1, text_2] for (text_1, text_2) in zip(examples['text_1'],examples['text_2'])])\n",
    "        tokenized_inputs = tokenizer(examples['text_1'],examples['text_2'], truncation=True)\n",
    "#         tokenized_inputs[\"labels\"] = [[labels2id[label] for _ in range(len(tokenized_inputs[\"input_ids\"][i]))] for i, label in enumerate(examples['class'])]\n",
    "        tokenized_inputs[\"labels\"] = [labels2id[label] for label in examples['class']]\n",
    "#         tokenized_inputs[\"labels\"] = [label for label in examples['class']]\n",
    "        return tokenized_inputs\n",
    "    return tokenize_and_align_labels_\n",
    "tokenized_datasets = data.map(tokenize_and_align_labels(tokenizer, labels2id), batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grayni\\AppData\\Local\\Temp\\ipykernel_13380\\1063226028.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"paraphras\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-2,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.05,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 4.00 GiB total capacity; 9.49 GiB already allocated; 0 bytes free; 9.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m----> 3\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\transformers\\trainer.py:1555\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1553\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   1554\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1555\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1556\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1557\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1558\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1559\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1560\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\transformers\\trainer.py:1837\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1834\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[0;32m   1836\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[1;32m-> 1837\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1839\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1840\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   1841\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[0;32m   1842\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   1843\u001B[0m ):\n\u001B[0;32m   1844\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   1845\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\transformers\\trainer.py:2693\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs)\u001B[0m\n\u001B[0;32m   2691\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m   2692\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2693\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2695\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:1905\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[1;34m(self, loss, **kwargs)\u001B[0m\n\u001B[0;32m   1903\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaler\u001B[38;5;241m.\u001B[39mscale(loss)\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1904\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1905\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 4.00 GiB total capacity; 9.49 GiB already allocated; 0 bytes free; 9.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_similarity(text1, text2):\n",
    "    \"\"\" Predict the probability that two Russian sentences are paraphrases of each other. \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        batch = tokenizer(\n",
    "            text1, text2,\n",
    "            truncation=True, max_length=model.config.max_position_embeddings, return_tensors='pt',\n",
    "        ).to(model.device)\n",
    "        proba = torch.softmax(model(**batch).logits, -1)\n",
    "    return proba[0][1].item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "0.002228879602625966"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"–Ø –Ω–µ –ø–æ–º–Ω—é —Ç–µ–±—è\"\n",
    "text2 = \"–Ø –Ω–µ –∑–Ω–∞—é —Ç–µ–±—è\"\n",
    "get_similarity(text1, text2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0017579307314008474"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"–¢–≤–æ–π –æ—Ç–µ—Ü –≥–æ–≤–æ—Ä–∏–ª, —á—Ç–æ –Ω–µ –ª—é–±–∏—Ç —Ñ—É—Ç–±–æ–ª\"\n",
    "text2 = \"–¢–≤–æ–π –æ—Ç–µ—Ü —Ä–∞—Å—Å–∫–∞–∑–∞–ª –æ —Å–≤–æ–µ–π –Ω–µ–ª—é–±–≤–∏ –∫ —Ñ—É—Ç–±–æ–ª—É\"\n",
    "get_similarity(text1, text2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0.002106076804921031"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"–ú–æ—è —Å–µ—Å—Ç—Ä–∞ –ª—é–±–∏—Ç –∂–∏–≤–æ—Ç–Ω—ã—Ö\"\n",
    "text2 = \"–ú–æ—è —Å–µ—Å—Ç—Ä–∞ –ª—é–±–∏—Ç —Å–æ–±–∞–∫—É\"\n",
    "get_similarity(text1, text2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0031429717782884836"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"–ú–æ—è —Å–µ—Å—Ç—Ä–∞ –ª—é–±–∏—Ç –∂–∏–≤–æ—Ç–Ω—ã—Ö\"\n",
    "text2 = \"–ú–æ—è —Å–µ—Å—Ç—Ä–∞ –ª—é–±–∏—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ Python\"\n",
    "get_similarity(text1, text2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0035726786591112614"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"–ú–æ—è —Å–µ—Å—Ç—Ä–∞ –ª—é–±–∏—Ç –∂–∏–≤–æ—Ç–Ω—ã—Ö\"\n",
    "text2 = \"–ú–æ—è —Å–µ—Å—Ç—Ä–∞ –ª—é–±–∏—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å\"\n",
    "get_similarity(text1, text2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               text_1  \\\n0                     –¶–µ–Ω—ã –Ω–∞ –Ω–µ—Ñ—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è   \n1   \"–ì–æ–≥–æ–ª—å-—Ü–µ–Ω—Ç—Ä\" –ø–æ–∫–∞–∂–µ—Ç –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—å —Å–∫–∞–Ω–¥–∞–ª—å–Ω–æ–≥...   \n2   –ê–≥–µ–Ω—Ç: –†–§–° –≤–Ω–æ–≤—å –∑–∞–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞—Ä–ø–ª–∞—Ç—É –§–∞–±–∏–æ –ö–∞...   \n3      –î–µ–Ω—å –ü–æ–±–µ–¥—ã –≤ –ú–æ—Å–∫–≤–µ –æ–±–µ—â–∞–µ—Ç –≤—ã–¥–∞—Ç—å—Å—è –æ–±–ª–∞—á–Ω—ã–º   \n4   –ü–æ—Å–æ–ª –†–§ –≤ –°–®–ê: –†–æ—Å—Å–∏—è –±—É–¥–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–æ–ø—ã—Ç–∫...   \n5   –í–µ—Ä—Ç–æ–ª–µ—Ç —Å 11 –∏–Ω–æ—Å—Ç—Ä–∞–Ω—Ü–∞–º–∏ –Ω–∞ –±–æ—Ä—Ç—É —É–ø–∞–ª –≤ –ü–∞–∫...   \n6   –°–∞–º–æ–ª–µ—Ç –≤–µ—Ä–Ω—É–ª—Å—è –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞ –∏–∑-–∑–∞...   \n7   –í–∞—Å–∏–ª—å–µ–≤–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –≤–∏–Ω–æ–≤–Ω–æ–π –≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–µ –∏ ...   \n8   –ü—É—Ç–∏–Ω –ø–µ—Ä–µ–¥ –î–Ω–µ–º –ü–æ–±–µ–¥—ã –ø–æ–¥–ø–∏—Å–∞–ª —É–∫–∞–∑ –æ –ø—Ä–∏—Å–≤–æ...   \n9   –°—É–¥ –æ–ø—Ä–∞–≤–¥–∞–ª –í–∞—Å–∏–ª—å–µ–≤—É –≤ —Ö–∏—â–µ–Ω–∏–∏ –∞–∫—Ü–∏–π –Ω–∞ –¥–≤–∞ ...   \n10  –ü—É—à–∫–æ–≤: —É –û–±–∞–º—ã –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ –¥—É—Ö–∞ –ª–∏—á–Ω–æ –ø–æ–∑–¥—Ä–∞–≤–∏...   \n11  –ú–ß–° –†–§: —Ç–µ–ª–∞ –ø–æ–≥–∏–±—à–∏—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∏–ø–ª–æ–º–∞—Ç–æ–≤ –¥–æ...   \n12  –î–µ–≤—è—Ç—å —Å–∞–º–æ–ª–µ—Ç–æ–≤ –í–í–° —Ä–∞–∑–≥–æ–Ω—è—Ç –æ–±–ª–∞–∫–∞ –Ω–∞–¥ –ú–æ—Å–∫–≤...   \n13       –¢—Ä–æ–∏—Ö –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤-—É–±–∏–π—Ü –ø–æ–π–º–∞–ª–∏ –≤ –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ   \n14  –°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–º–∏—Ç–µ—Ç —Å–æ–æ–±—â–∏–ª –æ –≤–∑–ª–æ–º–µ —Å–≤–æ–µ–≥–æ —Å...   \n\n                                               text_2 class predict  \n0   –ü–∞—Ä–ª–∞–º–µ–Ω—Ç –°–ª–æ–≤–∞–∫–∏–∏ –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏–ª –Ω–∞—Ä–æ–¥—ã –±—ã–≤—à–µ–≥–æ...    -1      -1  \n1   –ö–µ—Ö–º–∞–Ω –∑–∞–ø—Ä–µ—Ç–∏–ª ¬´–ì–æ–≥–æ–ª—å-—Ü–µ–Ω—Ç—Ä—É¬ª –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤–∏–¥...    -1      -1  \n2   –°–ú–ò: –ê–≥–µ–Ω—Ç –§–∞–±–∏–æ –ö–∞–ø–µ–ª–ª–æ –≥—Ä–æ–∑–∏—Ç—Å—è –ø–æ–¥–∞—Ç—å –≤ —Å—É–¥...    -1      -1  \n3    –õ—é–±–ª—è–Ω–∞ –æ—Ç–ø—Ä–∞–∑–¥–Ω—É–µ—Ç –î–µ–Ω—å –ü–æ–±–µ–¥—ã –≤–º–µ—Å—Ç–µ —Å –ú–æ—Å–∫–≤–æ–π    -1      -1  \n4   –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –ª–æ—Ç–µ...    -1      -1  \n5         –í –ü–∞–∫–∏—Å—Ç–∞–Ω–µ —É–ø–∞–ª –≤–µ—Ä—Ç–æ–ª–µ—Ç —Å 11 –∏–Ω–æ—Å—Ç—Ä–∞–Ω—Ü–∞–º–∏     1      -1  \n6   –°–∞–º–æ–ª–µ—Ç –≤–µ—Ä–Ω—É–ª—Å—è –≤ –Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∏–π –∞—ç—Ä–æ–ø–æ—Ä—Ç –∏–∑-–∑...     1      -1  \n7   –í–∞—Å–∏–ª—å–µ–≤–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –≤–∏–Ω–æ–≤–Ω–æ–π –≤ —Ö–∏—â–µ–Ω–∏—è—Ö –∏ –æ—Ç–º—ã–≤...     0      -1  \n8   –°–ö –†–§: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–º–∏—Ç–µ—Ç–∞...    -1      -1  \n9   –°—É–¥ –æ–ø—Ä–∞–≤–¥–∞–ª –í–∞—Å–∏–ª—å–µ–≤—É –≤ —Ö–∏—â–µ–Ω–∏–∏ –∞–∫—Ü–∏–π –Ω–∞ 2 –º–ª...     1      -1  \n10  –ü—É—à–∫–æ–≤: –û–±–∞–º–∞ –Ω–µ –Ω–∞—à–µ–ª –≤ —Å–µ–±–µ –¥—É—Ö–∞ –ª–∏—á–Ω–æ –ø–æ–∑–¥—Ä...     1      -1  \n11  –¢–µ–ª–∞ –ø–æ–≥–∏–±—à–∏—Ö –≤ –ù–µ–ø–∞–ª–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∏–ø–ª–æ–º–∞—Ç–æ–≤ –¥...     1      -1  \n12  –û–±–ª–∞–∫–∞ –Ω–∞–¥ –ú–æ—Å–∫–≤–æ–π –≤ –î–µ–Ω—å –ü–æ–±–µ–¥—ã —Ä–∞–∑–≥–æ–Ω—è—Ç –¥–µ–≤—è...     1      -1  \n13  –í –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ —Ç—Ä–æ–µ –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤ –ø—Ä–∏–∑–Ω–∞–ª–∏—Å—å –≤ —Å–µ—Ä...     0      -1  \n14   –í –°–ö–† —Å–æ–æ–±—â–∏–ª–∏ –æ –≤–∑–ª–æ–º–µ —Ö–∞–∫–µ—Ä–∞–º–∏ —Å–∞–π—Ç–∞ –≤–µ–¥–æ–º—Å—Ç–≤–∞     1      -1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_1</th>\n      <th>text_2</th>\n      <th>class</th>\n      <th>predict</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>–¶–µ–Ω—ã –Ω–∞ –Ω–µ—Ñ—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è</td>\n      <td>–ü–∞—Ä–ª–∞–º–µ–Ω—Ç –°–ª–æ–≤–∞–∫–∏–∏ –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏–ª –Ω–∞—Ä–æ–¥—ã –±—ã–≤—à–µ–≥–æ...</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"–ì–æ–≥–æ–ª—å-—Ü–µ–Ω—Ç—Ä\" –ø–æ–∫–∞–∂–µ—Ç –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—å —Å–∫–∞–Ω–¥–∞–ª—å–Ω–æ–≥...</td>\n      <td>–ö–µ—Ö–º–∞–Ω –∑–∞–ø—Ä–µ—Ç–∏–ª ¬´–ì–æ–≥–æ–ª—å-—Ü–µ–Ω—Ç—Ä—É¬ª –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤–∏–¥...</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>–ê–≥–µ–Ω—Ç: –†–§–° –≤–Ω–æ–≤—å –∑–∞–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞—Ä–ø–ª–∞—Ç—É –§–∞–±–∏–æ –ö–∞...</td>\n      <td>–°–ú–ò: –ê–≥–µ–Ω—Ç –§–∞–±–∏–æ –ö–∞–ø–µ–ª–ª–æ –≥—Ä–æ–∑–∏—Ç—Å—è –ø–æ–¥–∞—Ç—å –≤ —Å—É–¥...</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>–î–µ–Ω—å –ü–æ–±–µ–¥—ã –≤ –ú–æ—Å–∫–≤–µ –æ–±–µ—â–∞–µ—Ç –≤—ã–¥–∞—Ç—å—Å—è –æ–±–ª–∞—á–Ω—ã–º</td>\n      <td>–õ—é–±–ª—è–Ω–∞ –æ—Ç–ø—Ä–∞–∑–¥–Ω—É–µ—Ç –î–µ–Ω—å –ü–æ–±–µ–¥—ã –≤–º–µ—Å—Ç–µ —Å –ú–æ—Å–∫–≤–æ–π</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>–ü–æ—Å–æ–ª –†–§ –≤ –°–®–ê: –†–æ—Å—Å–∏—è –±—É–¥–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–æ–ø—ã—Ç–∫...</td>\n      <td>–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –ª–æ—Ç–µ...</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>–í–µ—Ä—Ç–æ–ª–µ—Ç —Å 11 –∏–Ω–æ—Å—Ç—Ä–∞–Ω—Ü–∞–º–∏ –Ω–∞ –±–æ—Ä—Ç—É —É–ø–∞–ª –≤ –ü–∞–∫...</td>\n      <td>–í –ü–∞–∫–∏—Å—Ç–∞–Ω–µ —É–ø–∞–ª –≤–µ—Ä—Ç–æ–ª–µ—Ç —Å 11 –∏–Ω–æ—Å—Ç—Ä–∞–Ω—Ü–∞–º–∏</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>–°–∞–º–æ–ª–µ—Ç –≤–µ—Ä–Ω—É–ª—Å—è –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞ –∏–∑-–∑–∞...</td>\n      <td>–°–∞–º–æ–ª–µ—Ç –≤–µ—Ä–Ω—É–ª—Å—è –≤ –Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∏–π –∞—ç—Ä–æ–ø–æ—Ä—Ç –∏–∑-–∑...</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>–í–∞—Å–∏–ª—å–µ–≤–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –≤–∏–Ω–æ–≤–Ω–æ–π –≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–µ –∏ ...</td>\n      <td>–í–∞—Å–∏–ª—å–µ–≤–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –≤–∏–Ω–æ–≤–Ω–æ–π –≤ —Ö–∏—â–µ–Ω–∏—è—Ö –∏ –æ—Ç–º—ã–≤...</td>\n      <td>0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>–ü—É—Ç–∏–Ω –ø–µ—Ä–µ–¥ –î–Ω–µ–º –ü–æ–±–µ–¥—ã –ø–æ–¥–ø–∏—Å–∞–ª —É–∫–∞–∑ –æ –ø—Ä–∏—Å–≤–æ...</td>\n      <td>–°–ö –†–§: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–º–∏—Ç–µ—Ç–∞...</td>\n      <td>-1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>–°—É–¥ –æ–ø—Ä–∞–≤–¥–∞–ª –í–∞—Å–∏–ª—å–µ–≤—É –≤ —Ö–∏—â–µ–Ω–∏–∏ –∞–∫—Ü–∏–π –Ω–∞ –¥–≤–∞ ...</td>\n      <td>–°—É–¥ –æ–ø—Ä–∞–≤–¥–∞–ª –í–∞—Å–∏–ª—å–µ–≤—É –≤ —Ö–∏—â–µ–Ω–∏–∏ –∞–∫—Ü–∏–π –Ω–∞ 2 –º–ª...</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>–ü—É—à–∫–æ–≤: —É –û–±–∞–º—ã –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ –¥—É—Ö–∞ –ª–∏—á–Ω–æ –ø–æ–∑–¥—Ä–∞–≤–∏...</td>\n      <td>–ü—É—à–∫–æ–≤: –û–±–∞–º–∞ –Ω–µ –Ω–∞—à–µ–ª –≤ —Å–µ–±–µ –¥—É—Ö–∞ –ª–∏—á–Ω–æ –ø–æ–∑–¥—Ä...</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>–ú–ß–° –†–§: —Ç–µ–ª–∞ –ø–æ–≥–∏–±—à–∏—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∏–ø–ª–æ–º–∞—Ç–æ–≤ –¥–æ...</td>\n      <td>–¢–µ–ª–∞ –ø–æ–≥–∏–±—à–∏—Ö –≤ –ù–µ–ø–∞–ª–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∏–ø–ª–æ–º–∞—Ç–æ–≤ –¥...</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>–î–µ–≤—è—Ç—å —Å–∞–º–æ–ª–µ—Ç–æ–≤ –í–í–° —Ä–∞–∑–≥–æ–Ω—è—Ç –æ–±–ª–∞–∫–∞ –Ω–∞–¥ –ú–æ—Å–∫–≤...</td>\n      <td>–û–±–ª–∞–∫–∞ –Ω–∞–¥ –ú–æ—Å–∫–≤–æ–π –≤ –î–µ–Ω—å –ü–æ–±–µ–¥—ã —Ä–∞–∑–≥–æ–Ω—è—Ç –¥–µ–≤—è...</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>–¢—Ä–æ–∏—Ö –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤-—É–±–∏–π—Ü –ø–æ–π–º–∞–ª–∏ –≤ –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ</td>\n      <td>–í –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ —Ç—Ä–æ–µ –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤ –ø—Ä–∏–∑–Ω–∞–ª–∏—Å—å –≤ —Å–µ—Ä...</td>\n      <td>0</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>–°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–º–∏—Ç–µ—Ç —Å–æ–æ–±—â–∏–ª –æ –≤–∑–ª–æ–º–µ —Å–≤–æ–µ–≥–æ —Å...</td>\n      <td>–í –°–ö–† —Å–æ–æ–±—â–∏–ª–∏ –æ –≤–∑–ª–æ–º–µ —Ö–∞–∫–µ—Ä–∞–º–∏ —Å–∞–π—Ç–∞ –≤–µ–¥–æ–º—Å—Ç–≤–∞</td>\n      <td>1</td>\n      <td>-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2labels = { id:key for id, key in enumerate(label_list)}\n",
    "example = tokenized_datasets[\"test\"][0:15]\n",
    "tokens = tokenizer(example['text_1'], example['text_1'], padding=True, truncation=True, return_tensors='pt')\n",
    "tokens = tokens.to('cuda:0')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "\n",
    "predicted = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "classes = [id2labels[id_label] for id_label in predicted]\n",
    "df_example =pd.DataFrame({'text_1':example['text_1'], 'text_2':example['text_2'], 'class':example['class'], 'predict':classes})\n",
    "df_example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}