{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.optim import Optimizer\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.1477,  1.1761,  0.2385],\n         [ 0.9800, -0.5217,  0.1854],\n         [-0.7014, -0.3157,  0.0058],\n         [-1.2235,  0.5164, -0.1806],\n         [-1.1853,  0.4897, -1.5347],\n         [-0.7726, -0.1140, -2.1939],\n         [-0.5408, -0.0099,  0.6673],\n         [ 2.4713,  0.6673,  1.4000],\n         [-0.7779,  0.5023, -1.3360],\n         [ 0.5957,  1.3773, -0.3514],\n         [-0.5962, -0.1911,  1.0068],\n         [-1.4420, -0.9462,  0.8203],\n         [ 2.5332,  1.6473,  0.2436],\n         [ 0.3981, -1.0504,  1.0002],\n         [-0.0571,  1.0098, -0.6806],\n         [-0.8278, -0.9173,  1.5652],\n         [ 1.1461, -0.2537, -1.3157],\n         [ 1.7280, -0.5511, -1.6518],\n         [ 0.2686, -0.5983,  0.9149],\n         [-0.1657, -0.5920,  1.4845],\n         [ 0.3906, -2.1320, -0.6200],\n         [ 1.7664, -1.2214,  0.3223],\n         [ 0.5007, -1.1563,  0.9554],\n         [-0.6403,  0.4780, -0.6033],\n         [ 0.9015,  0.7944, -0.2937],\n         [-0.4114, -0.6812, -1.9313],\n         [ 0.4770, -0.7149, -1.6948],\n         [-0.4208,  0.4077,  0.9501],\n         [ 1.3141, -0.4731,  0.3889],\n         [-0.2049,  1.3220, -1.5948],\n         [ 0.1388, -1.5555,  0.7670],\n         [ 0.0387,  0.3980, -1.4535],\n         [ 0.1552, -1.3233, -0.3651],\n         [-0.2648, -1.1856,  1.1217],\n         [ 1.0837, -1.1070,  1.5280],\n         [ 0.8595, -0.5481,  1.8854],\n         [ 0.9033, -1.0189,  0.5309],\n         [-0.0404,  2.0447, -0.0436],\n         [-0.0252,  0.5469, -0.7312],\n         [ 0.2377,  0.1548, -0.7548],\n         [ 0.0789, -0.0878,  1.0809],\n         [-0.7399,  0.7941, -1.9804],\n         [-0.7883,  1.2290,  1.4342],\n         [-2.0195, -0.6331, -1.2100],\n         [-0.7547,  1.0321,  0.9883],\n         [-1.0442, -0.6530,  0.2524],\n         [ 1.2000,  2.0460,  0.0763],\n         [-0.1298, -1.3573, -0.8737],\n         [ 0.7486, -1.1178,  1.8517],\n         [ 0.2165, -0.3428,  0.9296],\n         [ 1.4331,  0.0937, -1.2499],\n         [ 0.8571, -0.0086,  0.9042],\n         [ 0.6137,  0.2700, -0.4242],\n         [ 1.3963, -0.8936, -0.3070],\n         [ 1.6636,  1.6917, -0.0649],\n         [-1.5515,  0.0486,  0.5044],\n         [ 1.3867,  1.4877,  1.1803],\n         [-0.5553, -1.8261,  1.2785],\n         [-2.0326, -0.6532, -0.5962],\n         [ 1.8295,  0.1487,  0.7391],\n         [ 1.9745,  0.3567,  0.4410],\n         [-0.3838,  1.7214, -0.7572],\n         [-0.1890, -1.1957, -0.6308],\n         [-1.0430, -0.6505, -1.0689],\n         [-0.6632,  0.2901, -0.1426],\n         [-0.1239, -0.3539, -0.4393],\n         [ 0.5183, -0.2305, -1.2003],\n         [-1.3216,  0.5085,  0.9150],\n         [-0.5055,  0.8550,  1.1094],\n         [-0.5030,  0.8496,  0.1837],\n         [-1.4800, -2.7096,  0.5042],\n         [-0.2900,  0.4935, -0.9072],\n         [-0.2341,  0.1709,  0.3441],\n         [-0.9762,  1.0974,  0.8138],\n         [-0.3152,  1.7843,  0.2389],\n         [ 0.1830,  0.0944, -0.0741],\n         [-0.9776,  0.8490, -0.2964],\n         [-0.0490, -0.0055, -0.4092],\n         [ 0.6748,  0.4688,  1.8617],\n         [ 0.5740,  0.5950, -0.2589],\n         [ 0.9908,  0.0942, -1.0715],\n         [ 0.3987,  0.6321, -1.3440],\n         [-1.6862,  0.7688, -0.8334],\n         [ 0.8307, -0.2812, -0.5138],\n         [-0.6157, -1.4807,  0.4089],\n         [ 1.2971,  0.4985, -1.2655],\n         [ 0.1927,  1.3040, -1.5130],\n         [-1.0106, -0.8198,  1.6502],\n         [-0.5338, -0.4081,  1.7997],\n         [-1.1806,  0.1561, -1.0656],\n         [ 0.7793, -1.4199,  0.2081],\n         [ 0.8034, -0.8055, -1.6568],\n         [-0.5750,  1.0232, -1.2249],\n         [ 0.0088, -0.5093,  0.5258],\n         [ 0.1748, -0.2148,  0.9673],\n         [ 0.3466,  0.1614,  0.3864],\n         [ 1.8702, -0.6530,  0.7430],\n         [ 0.1779, -0.4930,  1.1391],\n         [-0.4205, -1.2827,  1.3180],\n         [-0.4055, -0.4106,  1.2857]]])"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn([1, 100, 3])\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.0725, -1.0301,  1.8985,  ..., -0.4468, -0.8776,  1.3075],\n         [-0.8638, -0.1594, -2.1663,  ...,  0.6597,  0.3633, -0.6196],\n         [ 0.5475, -0.0219, -0.9264,  ..., -1.0786,  1.7337, -0.9595],\n         ...,\n         [-0.2996,  1.1045,  1.3370,  ...,  1.2970, -0.1408, -0.1617],\n         [ 0.1590, -1.5545, -1.7229,  ..., -0.8568,  0.2522, -0.8653],\n         [ 0.0266, -2.0725,  0.3277,  ..., -0.3328,  0.7427, -0.5293]]])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.randn([1, 100, 64])\n",
    "target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(3, 64)  # w*x+b, w - weight, b - bias\n",
    "\n",
    "    def forward(self, x):  # __call__ -> forward()\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "model = Net()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 6.4221e-02,  5.8706e-01,  1.0781e-03,  ...,  4.2527e-01,\n           6.9483e-01,  8.0912e-01],\n         [ 6.1115e-01, -1.2252e-01, -1.3593e+00,  ...,  5.1047e-01,\n          -5.1240e-01,  4.1108e-01],\n         [ 4.0667e-01, -4.2657e-01, -3.0717e-01,  ...,  5.8798e-01,\n           1.9267e-01,  2.1620e-01],\n         ...,\n         [ 8.2103e-01, -9.1886e-02, -1.4527e+00,  ...,  1.0664e+00,\n           1.7230e-01,  7.2662e-02],\n         [ 1.0773e+00, -6.0008e-01, -1.7030e+00,  ...,  1.3178e+00,\n           7.6989e-02, -3.3255e-01],\n         [ 8.0428e-01, -1.4907e-01, -1.1929e+00,  ...,  1.1946e+00,\n           4.9957e-01, -3.6078e-02]]], grad_fn=<ViewBackward0>)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 100, 64])"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.0003, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False, *, maximize=False, foreach: Optional[bool] = None):\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov,\n",
    "                        maximize=maximize, foreach=foreach)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "            group.setdefault('maximize', False)\n",
    "            group.setdefault('foreach', None)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            momentum_buffer_list = []\n",
    "            has_sparse_grad = False\n",
    "\n",
    "            for p in group['params']:\n",
    "                print('Param:', p)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "                    if p.grad.is_sparse:\n",
    "                        has_sparse_grad = True\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        momentum_buffer_list.append(None)\n",
    "                    else:\n",
    "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
    "\n",
    "            sgd(params_with_grad,\n",
    "                d_p_list,\n",
    "                momentum_buffer_list,\n",
    "                weight_decay=group['weight_decay'],\n",
    "                momentum=group['momentum'],\n",
    "                lr=group['lr'],\n",
    "                dampening=group['dampening'],\n",
    "                nesterov=group['nesterov'],\n",
    "                maximize=group['maximize'],\n",
    "                has_sparse_grad=has_sparse_grad,\n",
    "                foreach=group['foreach'])\n",
    "\n",
    "            # update momentum_buffers in state\n",
    "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "                state = self.state[p]\n",
    "                state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss\n",
    "\n",
    "def sgd(params: List[Tensor],\n",
    "        d_p_list: List[Tensor],\n",
    "        momentum_buffer_list: List[Optional[Tensor]],\n",
    "        # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627\n",
    "        # setting this as kwarg for now as functional API is compiled by torch/distributed/optim\n",
    "        has_sparse_grad: bool = None,\n",
    "        foreach: bool = None,\n",
    "        *,\n",
    "        weight_decay: float,\n",
    "        momentum: float,\n",
    "        lr: float,\n",
    "        dampening: float,\n",
    "        nesterov: bool,\n",
    "        maximize: bool):\n",
    "    r\"\"\"Functional API that performs SGD algorithm computation.\n",
    "\n",
    "    See :class:`~torch.optim.SGD` for details.\n",
    "    \"\"\"\n",
    "\n",
    "    if foreach is None:\n",
    "        # Placeholder for more complex foreach logic to be added when value is not set\n",
    "        foreach = False\n",
    "\n",
    "    if foreach and torch.jit.is_scripting():\n",
    "        raise RuntimeError('torch.jit.script not supported with foreach optimizers')\n",
    "\n",
    "    if foreach and not torch.jit.is_scripting():\n",
    "        func = _multi_tensor_sgd\n",
    "    else:\n",
    "        func = _single_tensor_sgd\n",
    "\n",
    "    func(params,\n",
    "         d_p_list,\n",
    "         momentum_buffer_list,\n",
    "         weight_decay=weight_decay,\n",
    "         momentum=momentum,\n",
    "         lr=lr,\n",
    "         dampening=dampening,\n",
    "         nesterov=nesterov,\n",
    "         has_sparse_grad=has_sparse_grad,\n",
    "         maximize=maximize)\n",
    "\n",
    "def _single_tensor_sgd(params: List[Tensor],\n",
    "                       d_p_list: List[Tensor],\n",
    "                       momentum_buffer_list: List[Optional[Tensor]],\n",
    "                       *,\n",
    "                       weight_decay: float,\n",
    "                       momentum: float,\n",
    "                       lr: float,\n",
    "                       dampening: float,\n",
    "                       nesterov: bool,\n",
    "                       maximize: bool,\n",
    "                       has_sparse_grad: bool):\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        d_p = d_p_list[i]\n",
    "        if weight_decay != 0:\n",
    "            d_p = d_p.add(param, alpha=weight_decay)\n",
    "\n",
    "        if momentum != 0:\n",
    "            buf = momentum_buffer_list[i]\n",
    "\n",
    "            if buf is None:\n",
    "                buf = torch.clone(d_p).detach()\n",
    "                momentum_buffer_list[i] = buf\n",
    "            else:\n",
    "                buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "\n",
    "            if nesterov:\n",
    "                d_p = d_p.add(buf, alpha=momentum)\n",
    "            else:\n",
    "                d_p = buf\n",
    "\n",
    "        alpha = lr if maximize else -lr\n",
    "        param.add_(d_p, alpha=alpha)\n",
    "\n",
    "\n",
    "def _multi_tensor_sgd(params: List[Tensor],\n",
    "                      grads: List[Tensor],\n",
    "                      momentum_buffer_list: List[Optional[Tensor]],\n",
    "                      *,\n",
    "                      weight_decay: float,\n",
    "                      momentum: float,\n",
    "                      lr: float,\n",
    "                      dampening: float,\n",
    "                      nesterov: bool,\n",
    "                      maximize: bool,\n",
    "                      has_sparse_grad: bool):\n",
    "\n",
    "    if len(params) == 0:\n",
    "        return\n",
    "\n",
    "    if has_sparse_grad is None:\n",
    "        has_sparse_grad = any([grad.is_sparse for grad in grads])\n",
    "\n",
    "    if weight_decay != 0:\n",
    "        grads = torch._foreach_add(grads, params, alpha=weight_decay)\n",
    "\n",
    "    if momentum != 0:\n",
    "        bufs = []\n",
    "\n",
    "        all_states_with_momentum_buffer = True\n",
    "        for i in range(len(momentum_buffer_list)):\n",
    "            if momentum_buffer_list[i] is None:\n",
    "                all_states_with_momentum_buffer = False\n",
    "                break\n",
    "            else:\n",
    "                bufs.append(momentum_buffer_list[i])\n",
    "\n",
    "        if all_states_with_momentum_buffer:\n",
    "            torch._foreach_mul_(bufs, momentum)\n",
    "            torch._foreach_add_(bufs, grads, alpha=1 - dampening)\n",
    "        else:\n",
    "            bufs = []\n",
    "            for i in range(len(momentum_buffer_list)):\n",
    "                if momentum_buffer_list[i] is None:\n",
    "                    buf = momentum_buffer_list[i] = torch.clone(grads[i]).detach()\n",
    "                else:\n",
    "                    buf = momentum_buffer_list[i]\n",
    "                    buf.mul_(momentum).add_(grads[i], alpha=1 - dampening)\n",
    "\n",
    "                bufs.append(buf)\n",
    "\n",
    "        if nesterov:\n",
    "            torch._foreach_add_(grads, bufs, alpha=momentum)\n",
    "        else:\n",
    "            grads = bufs\n",
    "\n",
    "    alpha = lr if maximize else -lr\n",
    "    if not has_sparse_grad:\n",
    "        torch._foreach_add_(params, grads, alpha=alpha)\n",
    "    else:\n",
    "        # foreach APIs dont support sparse\n",
    "        for i in range(len(params)):\n",
    "            params[i].add_(grads[i], alpha=alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "optimizer = SGD(model.parameters(), lr=0.0003)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  1.4261630773544312\n",
      "Param: Parameter containing:\n",
      "tensor([[ 0.0550, -0.3038,  0.2754],\n",
      "        [ 0.2229,  0.5208,  0.2039],\n",
      "        [-0.4984,  0.5737, -0.5341],\n",
      "        [-0.3640, -0.0862, -0.4289],\n",
      "        [-0.3080, -0.3587,  0.1133],\n",
      "        [ 0.2690, -0.2616,  0.2324],\n",
      "        [ 0.5185, -0.4858,  0.4483],\n",
      "        [ 0.2765,  0.3116, -0.3453],\n",
      "        [ 0.4495,  0.0807,  0.5607],\n",
      "        [ 0.5556,  0.5078,  0.4169],\n",
      "        [ 0.0366,  0.3227, -0.1193],\n",
      "        [ 0.4773,  0.0684,  0.4565],\n",
      "        [-0.3486, -0.5000,  0.0359],\n",
      "        [-0.1063,  0.5527,  0.0112],\n",
      "        [ 0.0617,  0.5012,  0.3767],\n",
      "        [-0.1005, -0.0750,  0.1014],\n",
      "        [-0.0017, -0.2673,  0.5201],\n",
      "        [ 0.4541,  0.0377, -0.1670],\n",
      "        [ 0.0227, -0.1711, -0.0209],\n",
      "        [-0.0901, -0.4105, -0.3948],\n",
      "        [ 0.3868, -0.0460,  0.3663],\n",
      "        [ 0.2310,  0.1717, -0.0698],\n",
      "        [-0.5369,  0.3327, -0.1426],\n",
      "        [ 0.1396,  0.0988,  0.3782],\n",
      "        [ 0.3268, -0.3317,  0.3050],\n",
      "        [ 0.0442,  0.1923, -0.3844],\n",
      "        [-0.2568,  0.5435, -0.3435],\n",
      "        [-0.4094, -0.3672,  0.5188],\n",
      "        [-0.1028, -0.0993, -0.1911],\n",
      "        [-0.1012,  0.1887,  0.5416],\n",
      "        [-0.2609,  0.4534, -0.0683],\n",
      "        [ 0.0021,  0.3403, -0.0192],\n",
      "        [-0.1216,  0.1145, -0.2143],\n",
      "        [-0.2744,  0.1180, -0.1799],\n",
      "        [ 0.4808,  0.2771,  0.4145],\n",
      "        [ 0.2573, -0.1061, -0.3794],\n",
      "        [ 0.3118,  0.5463, -0.3197],\n",
      "        [ 0.3841, -0.2179, -0.2656],\n",
      "        [ 0.4992, -0.3239, -0.4933],\n",
      "        [-0.0456,  0.4271,  0.1650],\n",
      "        [ 0.4892, -0.3529, -0.2637],\n",
      "        [ 0.2947, -0.5233,  0.2482],\n",
      "        [-0.0908,  0.2792, -0.0649],\n",
      "        [-0.3166, -0.3635,  0.3434],\n",
      "        [-0.2333,  0.0031,  0.5563],\n",
      "        [-0.3765,  0.0639,  0.3162],\n",
      "        [-0.1957,  0.5039, -0.4822],\n",
      "        [ 0.1449,  0.3806, -0.5104],\n",
      "        [ 0.2983, -0.4150, -0.4967],\n",
      "        [ 0.0519,  0.2814, -0.5108],\n",
      "        [-0.1684,  0.5136, -0.3943],\n",
      "        [-0.0660, -0.4871, -0.2656],\n",
      "        [ 0.4446,  0.1368, -0.3611],\n",
      "        [-0.0138,  0.0982,  0.1464],\n",
      "        [ 0.5383,  0.5703, -0.3883],\n",
      "        [-0.3771,  0.0933,  0.3969],\n",
      "        [ 0.1974, -0.4656, -0.3581],\n",
      "        [ 0.2018,  0.4697,  0.3042],\n",
      "        [-0.0335, -0.3661, -0.4838],\n",
      "        [-0.5275,  0.0174, -0.2714],\n",
      "        [-0.4385, -0.1814,  0.1935],\n",
      "        [-0.1134, -0.1211,  0.4911],\n",
      "        [-0.3968,  0.5049,  0.3689],\n",
      "        [ 0.1791,  0.3289, -0.2141]], requires_grad=True)\n",
      "Param: Parameter containing:\n",
      "tensor([ 0.3477, -0.1070, -0.4725, -0.2580,  0.2438, -0.2703, -0.4660, -0.1695,\n",
      "         0.3161,  0.2726, -0.1220,  0.1495,  0.2606, -0.4317,  0.2089, -0.2433,\n",
      "        -0.1770, -0.2578, -0.3270,  0.2432, -0.4998, -0.0840, -0.3334,  0.1176,\n",
      "         0.3932,  0.1627, -0.0200,  0.5525, -0.4845, -0.2579,  0.4496,  0.3904,\n",
      "         0.2288,  0.1774, -0.0428, -0.4861,  0.0644,  0.4890,  0.3415, -0.3858,\n",
      "        -0.3778, -0.4604,  0.1055,  0.0253,  0.4267, -0.0849, -0.3389,  0.0011,\n",
      "         0.4792, -0.0677, -0.0132, -0.5177,  0.1187, -0.3669, -0.0864,  0.3541,\n",
      "         0.4239, -0.4455, -0.0750, -0.0390, -0.5473,  0.4673,  0.0716,  0.4468],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    output = model(x)\n",
    "    loss = criterion(output, target)\n",
    "    print('Epoch: ', epoch, 'Loss: ', loss.item())\n",
    "    optimizer.zero_grad()  # w.grad, b.grad - reset to zero\n",
    "    loss.backward()  # update w.grad, b.grad\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}