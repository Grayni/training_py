{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 26s 2us/step\n",
      "Len train: 25000\n",
      "Len test: 25000\n",
      "Pad sequences...\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Model...\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loading...\n",
      "Training...\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 141s 701ms/step - loss: 0.4389 - accuracy: 0.7858 - val_loss: 0.3435 - val_accuracy: 0.8508\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 139s 711ms/step - loss: 0.2798 - accuracy: 0.8856 - val_loss: 0.3420 - val_accuracy: 0.8510\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 147s 749ms/step - loss: 0.2231 - accuracy: 0.9128 - val_loss: 0.3647 - val_accuracy: 0.8444\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 151s 772ms/step - loss: 0.1733 - accuracy: 0.9344 - val_loss: 0.4583 - val_accuracy: 0.8390\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 149s 759ms/step - loss: 0.1395 - accuracy: 0.9478 - val_loss: 0.4728 - val_accuracy: 0.8360\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 150s 767ms/step - loss: 0.1189 - accuracy: 0.9555 - val_loss: 0.5751 - val_accuracy: 0.8335\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 158s 807ms/step - loss: 0.0972 - accuracy: 0.9654 - val_loss: 0.5606 - val_accuracy: 0.8134\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 162s 828ms/step - loss: 0.0882 - accuracy: 0.9684 - val_loss: 0.5933 - val_accuracy: 0.8260\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 158s 808ms/step - loss: 0.0681 - accuracy: 0.9754 - val_loss: 0.6963 - val_accuracy: 0.8210\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 155s 792ms/step - loss: 0.0662 - accuracy: 0.9767 - val_loss: 0.6663 - val_accuracy: 0.8233\n",
      "196/196 [==============================] - 15s 76ms/step - loss: 0.6663 - accuracy: 0.8233\n",
      "Loss: 0.666258692741394, Accuracy: 0.8232799768447876\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "\n",
    "# top 100\n",
    "maxlen = 100\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def train_nn():\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) # 10000\n",
    "    print(f'Len train: {len(x_train)}')\n",
    "    print(f'Len test: {len(x_test)}')\n",
    "\n",
    "    print('Pad sequences...')\n",
    "    x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "    print(f'x_train shape: {x_train.shape}')\n",
    "    print(f'x_test shape: {x_test.shape}')\n",
    "\n",
    "    print('Model...')\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    model.add(LSTM(128, dropout=0.4, recurrent_dropout=0.4))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print('Loading...')\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Training...')\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "    score, acc = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    print(f'Loss: {score}, Accuracy: {acc}')\n",
    "\n",
    "train_nn()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, ConvLSTM1D\n",
    "\n",
    "batch_size = 128\n",
    "maxlen = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "len train: 25000\n",
      "len test: 25000\n",
      "Pad sequences...\n",
      "x_train shape: (25000, 128)\n",
      "x_test shape: (25000, 128)\n",
      "Model...\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Training...\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 182s 916ms/step - loss: 0.4981 - accuracy: 0.7475 - val_loss: 0.3330 - val_accuracy: 0.8569\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 169s 864ms/step - loss: 0.2989 - accuracy: 0.8840 - val_loss: 0.3537 - val_accuracy: 0.8486\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 166s 850ms/step - loss: 0.2320 - accuracy: 0.9119 - val_loss: 0.3992 - val_accuracy: 0.8238\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 173s 881ms/step - loss: 0.1907 - accuracy: 0.9316 - val_loss: 0.3838 - val_accuracy: 0.8519\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 168s 860ms/step - loss: 0.1503 - accuracy: 0.9466 - val_loss: 0.4798 - val_accuracy: 0.8346\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 165s 842ms/step - loss: 0.1250 - accuracy: 0.9552 - val_loss: 0.4871 - val_accuracy: 0.8452\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 166s 850ms/step - loss: 0.1051 - accuracy: 0.9631 - val_loss: 0.5696 - val_accuracy: 0.8443\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 163s 830ms/step - loss: 0.0876 - accuracy: 0.9710 - val_loss: 0.5537 - val_accuracy: 0.8316\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 165s 842ms/step - loss: 0.0814 - accuracy: 0.9714 - val_loss: 0.6176 - val_accuracy: 0.8406\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 165s 843ms/step - loss: 0.0721 - accuracy: 0.9766 - val_loss: 0.8038 - val_accuracy: 0.8429\n",
      "196/196 [==============================] - 16s 81ms/step - loss: 0.8038 - accuracy: 0.8429\n",
      "loss: 0.8037581443786621, accuracy: 0.8428800106048584\n"
     ]
    }
   ],
   "source": [
    "def train_nn_1():\n",
    "    print('Loading...')\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "    print(f'len train: {len(x_train)}')\n",
    "    print(f'len test: {len(x_test)}')\n",
    "\n",
    "    print('Pad sequences...')\n",
    "    x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "    print(f'x_train shape: {x_train.shape}')\n",
    "    print(f'x_test shape: {x_test.shape}')\n",
    "\n",
    "    print('Model...')\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    model.add(LSTM(128, dropout=0.4, recurrent_dropout=0.4))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(32, activation='LeakyReLU'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(16, activation='elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='Adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Training...')\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=10,\n",
    "              validation_data=(x_test, y_test)),\n",
    "\n",
    "    score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "    print(f'loss: {score}, accuracy: {acc}')\n",
    "\n",
    "train_nn_1()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}